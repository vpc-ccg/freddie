import re
import subprocess
import os
import itertools
import glob 

import numpy as np
import pyfasta
import pysam

configfile: 'config.yaml'
localrules:
    all,
    make_time,
    sampled_gtf,
    tool_bed,
    truth_and_isoform_baselines,
    isoform_baseline_align_jobs,
    freddie,

for k in config.keys():
    if not k.startswith('override_'):
        continue
    keys = k[len('override_'):].split('_')
    top_dict = eval('config{}'.format(''.join(['["{}"]'.format(x) for x in keys[:-1]])))
    assert keys[-1] in top_dict
    top_dict[keys[-1]]=config[k]

ref_contigs = dict()
for species in config['references']:
    ref_contigs[species]=set()
    for line in open(config['references'][species]['genome_fai']):
        ref_contigs[species].add(line.rstrip().split()[0])

sample_tids = dict()
real_samples = list()
simu_samples = list()
for s in config['samples']:
    if config['samples'][s]['reads_info'] == 0:
        real_samples.append(s)
        continue
    simu_samples.append(s)
    tid_to_contig = dict()
    for l in open(config['references'][config['samples'][s]['ref']]['annot']):
        if l[0]=='#':
            continue
        l = l.rstrip().split('\t')
        if l[2]!='transcript':
            continue
        info = l[8]
        info = [x.strip().split(' ') for x in info.strip(';').split(';')]
        info = {x[0]:x[1].strip('"') for x in info}
        contig = l[0]
        tid = info['transcript_id']
        tid_to_contig[tid] = contig
    sample_tids[s] = dict()
    
    reads_info_file = open(config['samples'][s]['reads_info'])
    reads_info_file.readline()
    for line in reads_info_file:
        tid = line.rstrip().split('\t')[1]
        contig = tid_to_contig[tid]
        if not contig in sample_tids[s]:
            sample_tids[s][contig] = dict()
        sample_tids[s][contig][tid] = sample_tids[s][contig].get(tid, 0) + 1
    for contig in sample_tids[s]:
        sample_tids[s][contig] = {k for k,v in sample_tids[s][contig].items() if v >= config['min_cov']}

outpath      = config['outpath'].rstrip('/')
preprocess_d = '{}/preprocess'.format(outpath)
workspace_d  = '{}/workspace'.format(outpath)
output_d     = '{}/output'.format(outpath)
graphs_d     = '{}/graphs'.format(outpath)

tools = list()
for tool in config['tools']:
    if tool == 'flair':
        for r in config['gtf_sample_rates']:
            tools.append('flair.{:.2f}'.format(r))
    else:
        tools.append(tool)

def interval_extract(L, r=20):
    length = len(L)
    i = 0
    while (i< length):
        low = L[i]
        while i <length-1 and L[i]+r >= L[i + 1]:
            i += 1
        high = L[i]
        yield [low, high]
        i += 1


rule all:
    input:
        expand('{}/{{sample}}.{{mapper}}.{{extension}}'.format(preprocess_d),
            sample=config['samples'],
            mapper=config['mappers'],
            extension=['bam','bam.bai']
        ),
        expand('{}/{{sample}}/gtime.tsv'.format(output_d), sample=config['samples']),
        expand('{}/{{sample}}/{{mapper}}/{{tool}}.isoforms.gtf'.format(output_d),
            tool=tools,
            mapper=config['mappers'],
            sample=config['samples'],
        ),
        expand('{}/{{sample}}/{{mapper}}/freddie/{{stage}}'.format(workspace_d),
            mapper=config['mappers'],
            sample=config['samples'],
            stage=['split','segment','cluster'],
        ),
        # expand('{}/{{sample}}/{{mapper}}/{{tool}}.transcripts'.format(graphs_d),
        #     tool=tools+[
        #         'genome',
        #         'segment',
        #         'truth'
        #     ],
        #     mapper=config['mappers'],
        #     sample=[s for s in config['samples'] if config['samples'][s]['reads_info']!=0],
        # ),
        # expand('{}/{{sample}}/{{mapper}}/{{tool}}.{{extension}}'.format(graphs_d),
        #     tool=tools+[
        #         # 'genome',
        #         # 'segment',
        #         'isoform',
        #         'truth',
        #     ],
        #     mapper=config['mappers'],
        #     sample=[s for s in config['samples'] if config['samples'][s]['reads_info']!=0],
        #     extension=['bed', 'overlaps.tsv'],
        # ),
        # expand('{}/{{sample}}/{{mapper}}/{{tool}}.{{extension}}'.format(graphs_d),
        #     tool=tools+[
        #         'troth'
        #     ],
        #     mapper=config['mappers'],
        #     sample=[s for s in config['samples'] if config['samples'][s]['reads_info']==0],
        #     extension=['transcripts', 'bed', 'overlaps.tsv'],
        # ),
        # [
        #     '{prefix}/{sample}/{mapper}/isoform_baseline/jobs/{contig}.txt'.format(
        #         prefix=workspace_d,
        #         sample=s,
        #         mapper=m,
        #         contig=c,
        #     ) for m in config['mappers'] for s in config['samples'] if config['samples'][s]['reads_info']!=0 for c in sample_tids[s] 
        # ],
        # [
        #     '{prefix}/{sample}/{mapper}/isoform_baseline/pafs/{contig}/done'.format(
        #         prefix=workspace_d,
        #         sample=s,
        #         mapper=m,
        #         contig=c,
        #     ) for m in config['mappers'] for s in config['samples'] if config['samples'][s]['reads_info']!=0 for c in sample_tids[s] 
        # ],
        # [
        #     '{prefix}/{sample}/{mapper}/isoform.transcripts/{contig}'.format(
        #         prefix=graphs_d,
        #         sample=s,
        #         mapper=m,
        #         contig=c,
        #     ) for m in config['mappers'] for s in config['samples'] if config['samples'][s]['reads_info']!=0 for c in sample_tids[s] 
        # ],


rule minimap2:
    input:
        reads  = lambda wildcards: config['samples'][wildcards.sample]['reads'],
        genome = lambda wildcards: config['references'][config['samples'][wildcards.sample]['ref']]['genome'],
    output:
        bam=protected('{}/{{sample}}.minimap2.bam'.format(preprocess_d)),
        bai=protected('{}/{{sample}}.minimap2.bam.bai'.format(preprocess_d)),
    threads:
        32
    resources:
        mem  = "128G",
        time = 1439,
    shell:
        'minimap2 -a -x splice -t {threads} {input.genome} {input.reads} | '
        '  samtools sort -T {output.bam}.tmp -m 2G -@ {threads} -O bam - > {output.bam} && '
        '  samtools index {output.bam} '

rule desalt_index:
    input:
        genome = lambda wildcards: config['references'][wildcards.species]['genome'],
    output:
        index  = directory('test/mapping/{species}.dna.desalt_idx'),
    wildcard_constraints:
        species = '|'.join(re.escape(s) for s in config['references']),
    conda:
        config['conda_env_names']['freddie']
    resources:
        mem  = "128G",
        time = 179,
    shell:
        'deSALT index {input.genome} {output.index}'

rule desalt:
    input:
        reads  = lambda wildcards: config['samples'][wildcards.sample]['reads'],
        index  = lambda wildcards: config['references'][config['samples'][wildcards.sample]['ref']]['desalt_idx'],
    output:
        sam=temp('{}/{{sample}}.desalt.sam'.format(preprocess_d)),
        bam=protected('{}/{{sample}}.desalt.bam'.format(preprocess_d)),
        bai=protected('{}/{{sample}}.desalt.bam.bai'.format(preprocess_d)),
    params:
        seq_type = lambda wildcards: config['samples'][wildcards.sample]['seq_type'],
    conda:
        config['conda_env_names']['freddie']
    threads:
        32
    resources:
        mem  = "128G",
        time = 179,
    shell:
        'deSALT aln -x {params.seq_type} --thread {threads} -f {output.sam}_temp -o {output.sam} {input.index} {input.reads} &&'
        '  samtools sort -T {output.bam}.tmp -m 2G -@ {threads} -O bam {output.sam} > {output.bam} && '
        '  samtools index {output.bam} '

rule make_time:
    output:
        time_tsv = '{}/{{sample}}/gtime.tsv'.format(output_d)
    run:
        outfile = open(output.time_tsv, 'w+')
        record = list()
        record.append('tool')
        record.append('mapper')
        record.append('real')
        record.append('user')
        record.append('memory')
        outfile.write('\t'.join(record))
        outfile.write('\n')
        outfile.close()

rule freddie_split:
    input:
        bam   = '{}/{{sample}}.{{mapper}}.bam'.format(preprocess_d),
        reads =  lambda wildcards: config['samples'][wildcards.sample]['reads'],
        time  = ancient('{}/{{sample}}/gtime.tsv'.format(output_d))
    output:
        split   = directory('{}/{{sample}}/{{mapper}}/freddie/split'.format(workspace_d)),
    conda:
        config['conda_env_names']['freddie']
    threads:
        32
    wildcard_constraints:
        mapper='desalt|minimap2'
    shell:
        'GNU_TIME=$(which time) && $GNU_TIME -f "freddie-split\\t{wildcards.mapper}\\t%e\\t%U\\t%M"    -a -o {input.time} '
        '  py/freddie_split.py   -t {threads} -b {input.bam} -o {output.split} -r {input.reads}'


rule freddie_segment:
    input:
        split   = '{}/{{sample}}/{{mapper}}/freddie/split'.format(workspace_d),
        time  = ancient('{}/{{sample}}/gtime.tsv'.format(output_d))
    output:
        segment = directory('{}/{{sample}}/{{mapper}}/freddie/segment'.format(workspace_d)),
    conda:
        config['conda_env_names']['freddie']
    threads:
        32
    wildcard_constraints:
        mapper='desalt|minimap2'
    shell:
        'GNU_TIME=$(which time) && $GNU_TIME -f "freddie-segment\\t{wildcards.mapper}\\t%e\\t%U\\t%M"  -a -o {input.time} '
        '  py/freddie_segment.py -t {threads} -s {input.split} -o {output.segment} '


rule freddie_cluster:
    input:
        segment = '{}/{{sample}}/{{mapper}}/freddie/segment'.format(workspace_d),
        time  = ancient('{}/{{sample}}/gtime.tsv'.format(output_d))
    output:
        cluster = directory('{}/{{sample}}/{{mapper}}/freddie/cluster'.format(workspace_d)),
    params:
        gurobi   = config['gurobi']['license'],
    conda:
        config['conda_env_names']['freddie']
    threads:
        32
    wildcard_constraints:
        mapper='desalt|minimap2'
    shell:
        'export GRB_LICENSE_FILE={params.gurobi} && '
        'GNU_TIME=$(which time) && $GNU_TIME -f "freddie-cluster\\t{wildcards.mapper}\\t%e\\t%U\\t%M"  -a -o {input.time} '
        ' py/freddie_cluster.py  -t {threads} -s {input.segment} -o {output.cluster} '

rule freddie_cons:
    input:
        split   = '{}/{{sample}}/{{mapper}}/freddie/split'.format(workspace_d),
        cluster = '{}/{{sample}}/{{mapper}}/freddie/cluster'.format(workspace_d),
        time  = ancient('{}/{{sample}}/gtime.tsv'.format(output_d))
    output:
        gtf     = protected('{}/{{sample}}/{{mapper}}/freddie.isoforms.gtf'.format(output_d)),
    params:
        gurobi   = config['gurobi']['license'],
    conda:
        config['conda_env_names']['freddie']
    threads:
        32
    wildcard_constraints:
        mapper='desalt|minimap2'
    shell:
        'GNU_TIME=$(which time) && $GNU_TIME -f "freddie-collapse\\t{wildcards.mapper}\\t%e\\t%U\\t%M" -a -o {input.time} '
        ' py/freddie_isoforms.py -t {threads} -s {input.split} -c {input.cluster} -o {output.gtf}'

rule stringtie:
    input:
        bam = '{}/{{sample}}.{{mapper}}.bam'.format(preprocess_d),
        time  = ancient('{}/{{sample}}/gtime.tsv'.format(output_d))
    output:
        gtf  = protected('{}/{{sample}}/{{mapper}}/stringtie.isoforms.gtf'.format(output_d)),
    wildcard_constraints:
        mapper='desalt|minimap2'
    params:
        gnu_time_fmt = lambda wildcards: '"stringtie\\t'+wildcards.mapper+'\\t%e\\t%U\\t%M"',
    conda:
        config['conda_env_names']['stringtie']
    shell:
        'GNU_TIME=$(which time) && $GNU_TIME -f {params.gnu_time_fmt} -a -o {input.time} '
        ' stringtie -p 1 -L -o {output.gtf} {input.bam}'

rule sampled_gtf:
    input:
        gtf = lambda wildcards: config['references'][config['samples'][wildcards.sample]['ref']]['annot'],
    output:
        gtf ='{}/{{sample}}.sampled_at.{{gtf_sample_rate}}.gtf'.format(preprocess_d),
    run:
        print(input.gtf)
        np.random.seed(42)
        isoform_ids = set()
        isoform_id_gid = dict()
        for l in open(input.gtf):
            if l[0]=='#':
                continue
            l = l.rstrip().split('\t')
            if l[2]!='transcript':
                continue
            info = l[8]
            info = [x.strip().split(' ') for x in info.strip(';').split(';')]
            info = {x[0]:x[1].strip('"') for x in info}
            isoform_ids.add(info['transcript_id'])
            isoform_id_gid[info['transcript_id']] = info['gene_id']
        print(len(isoform_ids))
        isoform_ids = set(np.random.choice(
            list(isoform_ids),
            int(len(isoform_ids)*float(wildcards.gtf_sample_rate)),
            replace=False,
        ))
        print(len(isoform_ids))
        gene_ids = {isoform_id_gid[isoform_id] for isoform_id in isoform_ids}
        print(len(gene_ids))
        out_file = open(output.gtf, 'w+')
        for line in open(input.gtf):
            if line[0]=='#':
                out_file.write(line)
                continue
            l = line.rstrip().split('\t')
            info = l[8]
            info = [x.strip().split(' ') for x in info.strip(';').split(';')]
            info = {x[0]:x[1].strip('"') for x in info}
            if l[2]=='gene' and info['gene_id'] in gene_ids:
                out_file.write(line)
            elif l[2]!='gene' and info['transcript_id'] in isoform_ids:
                out_file.write(line)
        out_file.close()

rule flair:
    input:
        flair  = config['exec']['flair'],
        b2b12  = config['exec']['bam2Bed12'],
        bam    = '{}/{{sample}}.{{mapper}}.bam'.format(preprocess_d),
        reads  = lambda wildcards: config['samples'][wildcards.sample]['reads'],
        genome = lambda wildcards: config['references'][config['samples'][wildcards.sample]['ref']]['genome'],
        gtf    = '{}/{{sample}}.sampled_at.{{gtf_sample_rate}}.gtf'.format(preprocess_d),
        time   = ancient('{}/{{sample}}/gtime.tsv'.format(output_d))
    output:
        p_bed  = protected('{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}/input.bed'.format(workspace_d)),
        c_bed  = protected('{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}/correct_all_corrected.bed'.format(workspace_d)),
        i_bed  = protected('{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}/correct_all_inconsistent.bed'.format(workspace_d)),
        t_bed  = protected('{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}/collapse.isoforms.bed'.format(workspace_d)),
        fasta  = protected('{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}/collapse.isoforms.fa'.format(workspace_d)),
        gtf  = protected('{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}.isoforms.gtf'.format(output_d)),
    params:
        gnu_time_fmt    = lambda wildcards: '"flair-'+wildcards.gtf_sample_rate+'\\t'+wildcards.mapper+'\\t%e\\t%U\\t%M"',
        correct_prefix  = '{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}/correct'.format(workspace_d),
        collapse_prefix = '{}/{{sample}}/{{mapper}}/flair.{{gtf_sample_rate}}/collapse'.format(workspace_d),
    threads:
        32
    conda:
        config['conda_env_names']['flair']
    wildcard_constraints:
        mapper='desalt|minimap2'
    shell:
        'GNU_TIME=$(which time) && $GNU_TIME -f {params.gnu_time_fmt} -a -o {input.time} bash -c "'
        ' python3 {input.b2b12} -i {input.bam} > {output.p_bed}  && '
        ' python3 {input.flair} correct  -t {threads} -q {output.p_bed} -o {params.correct_prefix} -g {input.genome} -f {input.gtf} &&'
        ' python3 {input.flair} collapse -t {threads} -q {output.c_bed} -o {params.collapse_prefix} -g {input.genome} -f {input.gtf} -r {input.reads} '
        '"; '
        'mv {params.collapse_prefix}.isoforms.gtf {output.gtf}'

rule tool_bed:
    input:
        gtf = '{}/{{sample}}/{{mapper}}/{{tool}}.isoforms.gtf'.format(output_d),
    output:
        bed = '{}/{{sample}}/{{mapper}}/{{tool}}.bed'.format(graphs_d)
    wildcard_constraints:
        tool = '|'.join(re.escape(t) for t in tools)
    run:
        tool_isoforms = dict()
        for l in open(input.gtf):
            if l[0]=='#':
                continue
            l = l.rstrip().split('\t')
            if l[2]!='exon':
                continue
            contig = l[0]
            if not contig in tool_isoforms:
                tool_isoforms[contig]=dict()
            info = l[8]
            info = [x.strip().split(' ') for x in info.strip(';').split(';')]
            info = {x[0]:x[1].strip('"') for x in info}
            iid = '{}_{}'.format(wildcards.tool, info['transcript_id'])
            if not iid in tool_isoforms[contig]:
                tool_isoforms[contig][iid] = list()
            tool_isoforms[contig][iid].append((int(l[3]),int(l[4])))
        for v1 in tool_isoforms.values():
            for v2 in v1.values():
                v2.sort()
        bed_file = open(output.bed, 'w+')
        for contig,isoforms in sorted(tool_isoforms.items()):
            for iid,intervals in isoforms.items():
                for s,e in intervals:
                    bed_file.write('{}\t{}\t{}\t{}\n'.format(contig, s, e, iid))
        bed_file.close()



rule truth_and_isoform_baselines:
    input:
        reads  = lambda wildcards: config['samples'][wildcards.sample]['reads'],
        gtf    = lambda wildcards: config['references'][config['samples'][wildcards.sample]['ref']]['annot'],
        genome = lambda wildcards: config['references'][config['samples'][wildcards.sample]['ref']]['genome'],
    output:
        isoform_baseline_ready = '{}/{{sample}}/{{mapper}}/isoform_baseline/ready'.format(workspace_d),
        truth_bed = '{}/{{sample}}/{{mapper}}/truth.bed'.format(graphs_d),
        isoform_bed = '{}/{{sample}}/{{mapper}}/isoform.bed'.format(graphs_d),
    params:
        isoform_baseline_dirs = '{}/{{sample}}/{{mapper}}/isoform_baseline/reads'.format(workspace_d),
    run:
        tid_intervals = dict() #{c:dict() for c in contig_to_key.keys()}
        tid_to_contig = dict()
        tid_to_reads = dict() #{c:dict() for c in contig_to_key.keys()}
        for l in open(input.gtf):
            if l[0]=='#':
                continue
            l = l.rstrip().split('\t')
            if l[2]!='exon':
                continue
            info = l[8]
            info = [x.strip().split(' ') for x in info.strip(';').split(';')]
            info = {x[0]:x[1].strip('"') for x in info}
            contig = l[0]
            if not contig in tid_to_reads:
                tid_intervals[contig] = dict()
                tid_to_reads[contig] = dict()
            tid = info['transcript_id']
            if not tid in tid_intervals[contig]:
                tid_to_contig[tid] = contig
                tid_intervals[contig][tid] = list()
            tid_intervals[contig][tid].append((int(l[3]),int(l[4])))
        for read_file in input.reads:
            if read_file.endswith('.fq') or read_file.endswith('.fastq'):
                num = 4 
            else:
                num = 2
            infile = open(read_file)
            while True:
                lines = [line.rstrip() for line in itertools.islice(infile, num)]
                if (len(lines)) != num:
                    break
                tid = lines[0][1:].split()[0].split('_')[0]
                contig = tid_to_contig[tid]
                if not tid in tid_to_reads[contig]:
                    tid_to_reads[contig][tid] = list()
                tid_to_reads[contig][tid].append(lines[0]+'\n'+lines[1]+'\n')
        truth_bed = open(output.truth_bed, 'w+')
        isoform_bed = open(output.isoform_bed, 'w+')
        for contig,tid_reads in tid_to_reads.items():
            prefix = '{}/{}'.format(params.isoform_baseline_dirs, contig)
            os.makedirs(prefix, exist_ok=True)
            for tid,reads in tid_reads.items():
                if len(reads) < config['min_cov']:
                    continue
                outfile = open('{}/{}_{}_reads.fa'.format(prefix, contig, tid), 'w+')
                for read in reads:
                    outfile.write(read)
                outfile.close()
                
                for s,e in sorted(tid_intervals[contig][tid]):
                    truth_bed.write('{}\t{}\t{}\ttruth_{}_{}\n'.format(contig, s, e, contig, tid))
                    isoform_bed.write('{}\t{}\t{}\tisoform_{}_{}\n'.format(contig, s, e, contig, tid))
        truth_bed.close()
        isoform_bed.close()
        open(output.isoform_baseline_ready, 'w+')


rule isoform_baseline_align_jobs:
    output:
        jobs = '{}/{{sample}}/{{mapper}}/isoform_baseline/jobs/{{contig}}.txt'.format(workspace_d),
    wildcard_constraints:
        contig = '|'.join(list({re.escape(c) for s in sample_tids for c in sample_tids[s]}))
    run:
        target_template = ''
        outfile = open(output.jobs, 'w+')
        p_tmplt = '{p}/{s}/{m}/isoform_baseline/pafs/{c}/{c}_{{t}}_reads.paf'.format(
            p=workspace_d,
            s=wildcards.sample,
            m=wildcards.mapper,
            c=wildcards.contig,
        )
        r_tmplt = '{p}/{s}/{m}/isoform_baseline/reads/{c}/{c}_{{t}}_reads.fa'.format(
            p=workspace_d,
            s=wildcards.sample,
            m=wildcards.mapper,
            c=wildcards.contig,
        )
        t_tmplt = '{p}/{s}/{m}/truth.transcripts/{c}/truth_{c}_{{t}}.fa'.format(
            p=graphs_d,
            s=wildcards.sample,
            m=wildcards.mapper,
            c=wildcards.contig,
        )
        for t in sample_tids[wildcards.sample][wildcards.contig]:
            outfile.write('minimap2 -t 1 --no-long-join {} {} > {} 2> /dev/null\n'.format(
                t_tmplt.format(t=t),
                r_tmplt.format(t=t),
                p_tmplt.format(t=t),
            ))
        outfile.close()

rule isoform_baseline_align:
    input:
        isoform_baseline_ready = '{}/{{sample}}/{{mapper}}/isoform_baseline/ready'.format(workspace_d),
        truth_contig_dirs = '{}/{{sample}}/{{mapper}}/truth.transcripts'.format(graphs_d),
        jobs = '{}/{{sample}}/{{mapper}}/isoform_baseline/jobs/{{contig}}.txt'.format(workspace_d),
    output:
        done = '{}/{{sample}}/{{mapper}}/isoform_baseline/pafs/{{contig}}/done'.format(workspace_d),
    wildcard_constraints:
        contig = '|'.join(list({re.escape(c) for s in sample_tids for c in sample_tids[s]}))
    threads:
        32
    resources:
        mem  = 1024,
        time = 360,
    shell:
        'parallel --jobs {threads} < {input.jobs} && touch {output.done}'

rule isoform_based_jobs:
    output:
        jobs = '{}/{{sample}}/{{mapper}}/isoform_baseline/cons_jobs/{{contig}}.txt'.format(workspace_d),
    wildcard_constraints:
        contig = '|'.join(list({re.escape(c) for s in sample_tids for c in sample_tids[s]}))
    run:
        target_template = ''
        outfile = open(output.jobs, 'w+')
        p_tmplt = '{p}/{s}/{m}/isoform_baseline/pafs/{c}/{c}_{{t}}_reads.paf'.format(
            p=workspace_d,
            s=wildcards.sample,
            m=wildcards.mapper,
            c=wildcards.contig,
        )
        o_tmplt = '{p}/{s}/{m}/isoform.transcripts/{c}/isoform_{c}_{{t}}.fa'.format(
            p=graphs_d,
            s=wildcards.sample,
            m=wildcards.mapper,
            c=wildcards.contig,
        )
        t_tmplt = '{p}/{s}/{m}/truth.transcripts/{c}/truth_{c}_{{t}}.fa'.format(
            p=graphs_d,
            s=wildcards.sample,
            m=wildcards.mapper,
            c=wildcards.contig,
        )
        for t in sample_tids[wildcards.sample][wildcards.contig]:
            outfile.write('python3 extern/isoform_based.py {} {} {} {} {}\n'.format(
                p_tmplt.format(t=t),
                t_tmplt.format(t=t),
                o_tmplt.format(t=t),
                wildcards.contig,
                t,
            ))
        outfile.close()

rule isoform_based:
    input:
        done = '{}/{{sample}}/{{mapper}}/isoform_baseline/pafs/{{contig}}/done'.format(workspace_d),
        isoform_baseline_ready = '{}/{{sample}}/{{mapper}}/isoform_baseline/ready'.format(workspace_d),
        truth_contig_dirs = '{}/{{sample}}/{{mapper}}/truth.transcripts'.format(graphs_d),
        jobs = '{}/{{sample}}/{{mapper}}/isoform_baseline/cons_jobs/{{contig}}.txt'.format(workspace_d),
    output:
        done = directory('{}/{{sample}}/{{mapper}}/isoform.transcripts/{{contig}}'.format(graphs_d)),
    wildcard_constraints:
        contig = '|'.join(list({re.escape(c) for s in sample_tids for c in sample_tids[s]}))
    threads:
        32
    resources:
        mem  = 1024,
        time = 360,
    shell:
        'mkdir -p {output.done} && parallel --jobs {threads} < {input.jobs}'

rule bed_overlaps_truth:
    input:
        tool_bed = '{}/{{sample}}/{{mapper}}/{{tool}}.bed'.format(graphs_d),
        truth_bed = '{}/{{sample}}/{{mapper}}/truth.bed'.format(graphs_d),
    output:
        overlaps_tsv = '{}/{{sample}}/{{mapper}}/{{tool}}.overlaps.tsv'.format(graphs_d),
    wildcard_constraints:
        sample = '$^|'+'|'.join(re.escape(s) for s in simu_samples)
    shell:
        'bedtools intersect -a {input.tool_bed} -b <(cat {input.tool_bed} {input.truth_bed}) -wa -wb | '
        ' cut -f1,4,8 | '
        ' sort -u > {output.overlaps_tsv}'

rule bed_merge:
    input:
        lambda wildcards: ['{}/{{sample}}/{{mapper}}/{{tool}}.{}.bed'.format(graphs_d,c) 
            for c in ref_contigs[config['samples'][wildcards.sample]['ref']]],
    output:
        '{}/{{sample}}/{{mapper}}/{{tool}}.bed'.format(graphs_d),
    wildcard_constraints:
        tool = 'genome|troth'
    shell:
        'cat {input} > {output}'


rule bed_to_fasta:
    input:
        bed = '{}/{{sample}}/{{mapper}}/{{tool}}.bed'.format(graphs_d),
        genome = lambda wildcards: config['references'][config['samples'][wildcards.sample]['ref']]['genome'],
    output:
        out_dir = directory('{}/{{sample}}/{{mapper}}/{{tool}}.transcripts'.format(graphs_d)),
    wildcard_constraints:
        tool = '|'.join(re.escape(t) for t in tools+['genome', 'segment', 'truth', 'troth'])
    run:
        dna = pyfasta.Fasta(input.genome)
        contig_to_key = {k.split()[0]:k for k in dna.keys()}
        for contig in contig_to_key.keys():
            os.makedirs('{}/{}'.format(output.out_dir, contig), exist_ok=False)
        tids = dict()
        for line in open(input.bed):
            contig,start,end,tid = line.rstrip().split('\t')
            if not tid in tids:
                tids[tid] = dict(
                    contig=contig,
                    intervals=list()
                )
            tids[tid]['intervals'].append((int(start),int(end)))
        for tid in tids:
            contig = tids[tid]['contig']
            outfile = open(f'{output.out_dir}/{contig}/{tid}.fa', 'w+')
            outfile.write(f'>{tid}\n')
            dna_key = contig_to_key[contig]
            for s,e in sorted(tids[tid]['intervals']):
                outfile.write(dna[dna_key][s:e])
            outfile.write('\n')
            outfile.close()

